# -*- coding: utf-8 -*-
"""Model1_HPFT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zeq3sYtlEwA8vBqjWspTqg83Ca_6J4D3
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover
from pyspark.ml.linalg import Vectors
from pyspark.ml.recommendation import ALS
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from textblob import TextBlob
import re
from collections import defaultdict

class HybridRecommender:
    def __init__(self):
        self.spark = SparkSession.builder \
            .appName("YelpHybridRecommender") \
            .config("spark.driver.memory", "4g") \
            .config("spark.executor.memory", "4g") \
            .getOrCreate()

        self.business_df = None
        self.user_df = None
        self.reviews_df = None
        self.tips_df = None
        self.checkin_df = None
        self.tfidf_model = None
        self.best_model = None
        self.sentiment_scores = None
        self.checkin_scores = None
        self.popular_items = None
        self.user_profiles = {}

    def load_and_preprocess_data(self, business_path, user_path, reviews_path, tips_path, checkin_path):
        """
        Load and preprocess all datasets
        """
        # Load data
        self.business_df = self.spark.read.csv(business_path, header=True, inferSchema=True)
        self.user_df = self.spark.read.csv(user_path, header=True, inferSchema=True)
        self.reviews_df = self.spark.read.csv(reviews_path, header=True, inferSchema=True)
        self.tips_df = self.spark.read.csv(tips_path, header=True, inferSchema=True)
        self.checkin_df = self.spark.read.csv(checkin_path, header=True, inferSchema=True)

        # Preprocess data
        self.preprocess_business_data()
        self.preprocess_user_data()
        self.calculate_sentiment_scores()
        self.calculate_checkin_scores()
        self.calculate_popular_items()
        self.create_user_profiles()

    def preprocess_business_data(self):
        """
        Preprocess business data
        """
        # Clean categories
        self.business_df = self.business_df.withColumn("processed_categories",
                                                       self.preprocess_text(col("categories")))

        # Calculate average ratings
        business_avg_ratings = self.reviews_df.groupBy("business_id") \
            .agg(avg("stars").alias("avg_rating"), count("*").alias("review_count"))
        self.business_df = self.business_df.join(business_avg_ratings, "business_id", "left")

        # Create business features
        self.business_df = self.business_df.withColumn("price_level",
                                                       size(split(col("attributes"), "\$")) - 1)

    def preprocess_user_data(self):
        """
        Preprocess user data
        """
        # Calculate user activity metrics
        user_activity = self.reviews_df.groupBy("user_id") \
            .agg(count("business_id").alias("review_count"),
                 avg("stars").alias("average_rating"))

        self.user_df = self.user_df.join(user_activity, "user_id", "left")

    def create_user_profiles(self):
        """
        Create user profiles based on their review history
        """
        user_profiles = self.reviews_df.join(self.business_df, "business_id") \
            .groupBy("user_id") \
            .agg(collect_list(struct("categories", "stars")).alias("reviews"))

        def create_profile(reviews):
            category_preferences = defaultdict(float)
            for review in reviews:
                categories = review.categories.split(", ")
                for category in categories:
                    category_preferences[category] += review.stars
            total_ratings = sum(category_preferences.values())
            if total_ratings > 0:
                category_preferences = {k: v/total_ratings for k, v in category_preferences.items()}
            return category_preferences

        create_profile_udf = udf(create_profile, MapType(StringType(), FloatType()))

        user_profiles = user_profiles.withColumn("category_preferences", create_profile_udf("reviews"))

        self.user_profiles = user_profiles.collect()

    def calculate_popular_items(self):
        """
        Calculate popular items for cold start recommendations
        """
        self.popular_items = self.business_df.join(self.sentiment_scores, "business_id", "left") \
            .join(self.checkin_scores, "business_id", "left") \
            .withColumn("popularity_score",
                        col("avg_rating") * 0.4 +
                        col("review_count") * 0.3 +
                        col("sentiment_score") * 0.2 +
                        col("checkin_score") * 0.1) \
            .orderBy(desc("popularity_score"))

    def handle_cold_start(self, user_id=None, business_id=None):
        """
        Handle cold start problems for new users or items
        """
        if user_id and user_id not in [profile.user_id for profile in self.user_profiles]:
            # New user: Return popular items
            return self.popular_items.limit(10)

        if business_id and business_id not in self.business_df.select("business_id").rdd.flatMap(lambda x: x).collect():
            # New item: Return similar items based on categories
            business_categories = self.business_df.filter(col("business_id") == business_id) \
                .select("categories").first().categories

            return self.get_content_based_recommendations(business_categories, n_recommendations=10)

        return None

    def find_best_collaborative_model(self):
        """
        Find the best collaborative filtering model using ALS
        """
        als = ALS(userCol="user_id", itemCol="business_id", ratingCol="stars",
                  coldStartStrategy="drop", nonnegative=True)

        paramGrid = ParamGridBuilder() \
            .addGrid(als.rank, [10, 50, 100]) \
            .addGrid(als.regParam, [0.01, 0.1, 1.0]) \
            .addGrid(als.maxIter, [5, 10, 20]) \
            .build()

        evaluator = RegressionEvaluator(metricName="rmse", labelCol="stars", predictionCol="prediction")

        cv = CrossValidator(estimator=als, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)

        model = cv.fit(self.reviews_df)

        self.best_model = model.bestModel

        return self.best_model, evaluator.evaluate(self.best_model.transform(self.reviews_df))

    def get_content_based_recommendations(self, categories, n_recommendations=10):
        """
        Get content-based recommendations based on business categories
        """
        tokenizer = Tokenizer(inputCol="processed_categories", outputCol="words")
        wordsData = tokenizer.transform(self.business_df.select("business_id", "processed_categories"))

        hashingTF = HashingTF(inputCol="words", outputCol="rawFeatures", numFeatures=20)
        featurizedData = hashingTF.transform(wordsData)

        idf = IDF(inputCol=" rawFeatures", outputCol="features")
        idfModel = idf.fit(featurizedData)
        tfidfData = idfModel.transform(featurizedData)

        # Create TF-IDF vector for the input categories
        input_vector = tfidfModel.transform(self.spark.createDataFrame([(categories,)], ["processed_categories"]))

        # Calculate cosine similarity with existing businesses
        cosine_sim = cosine_similarity(input_vector.select("features").rdd.flatMap(lambda x: x).collect(),
                                       tfidfData.select("features").rdd.flatMap(lambda x: x).collect())

        # Get top N recommendations
        sim_scores = list(enumerate(cosine_sim[0]))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
        top_indices = [i[0] for i in sim_scores[:n_recommendations]]

        return self.business_df.select("business_id").collect()[top_indices]

    def preprocess_text(self, text):
        """
        Preprocess text data
        """
        if isinstance(text, str):
            # Convert to lowercase
            text = text.lower()
            # Remove special characters and digits
            text = re.sub(r'[^a-zA-Z\s]', '', text)
            return text
        return ''

    def calculate_sentiment_scores(self):
        """
        Calculate sentiment scores for reviews and tips
        """
        # Process reviews
        self.reviews_df = self.reviews_df.withColumn("sentiment",
            udf(lambda x: TextBlob(str(x)).sentiment.polarity, FloatType())(col("text")))

        # Process tips
        self.tips_df = self.tips_df.withColumn("sentiment",
            udf(lambda x: TextBlob(str(x)).sentiment.polarity, FloatType())(col("text")))

        # Aggregate sentiment scores by business
        review_sentiments = self.reviews_df.groupBy("business_id").agg(avg("sentiment").alias("review_sentiment"))
        tip_sentiments = self.tips_df.groupBy("business_id").agg(avg("sentiment").alias("tip_sentiment"))

        # Combine sentiment scores
        self.sentiment_scores = review_sentiments.join(tip_sentiments, "business_id", "outer").fillna(0)

    def calculate_checkin_scores(self):
        """
        Calculate check-in scores for businesses
        """
        # A checkin_df has columns: business_id, date
        self.checkin_df = self.checkin_df.withColumn("date", to_date(col("date")))

        # Calculate recency and frequency
        now = datetime.now()
        checkin_stats = self.checkin_df.groupBy("business_id").agg(
            max("date").alias("last_checkin"),
            count("date").alias("frequency")
        )

        checkin_stats = checkin_stats.withColumn("recency", datediff(lit(now), col("last_checkin")))

        # Normalize scores
        scaler = MinMaxScaler(inputCol="frequency", outputCol="frequency_scaled")
        self.checkin_scores = scaler.fit(checkin_stats).transform(checkin_stats)

        # Invert recency so that lower values (more recent) get higher scores
        self.checkin_scores = self.checkin_scores.withColumn("recency_scaled", 1 - (col("recency") / col("recency").max()))

# Example usage
recommender = HybridRecommender()
recommender.load_and_preprocess_data('business.csv', 'user.csv', 'reviews.csv', 'tips.csv', 'checkin.csv')
best_model, model_results = recommender.find_best_collaborative_model()
print(f"Best Collaborative Model: {best_model}, Results: {model_results}")