# -*- coding: utf-8 -*-
"""Recommendation_System_M1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lG9ErjjQRiohxj_1xczsjZ3EAeikmNWW
"""

! pip install pyspark

from google.colab import drive
drive.mount('/content/drive')

!pip install textblob

!pip install lightfm
!pip install gensim

# Create a Spark session
spark = SparkSession.builder.appName("HybridRecommendationSystem").getOrCreate()

# Import necessary libraries
from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from textblob import TextBlob
from pyspark.ml.feature import Word2Vec
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.recommendation import ALS
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.feature import Normalizer
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.feature import StringIndexer, VectorIndexer
from pyspark.ml.regression import LinearRegression
import numpy as np
from scipy.sparse import coo_matrix
from gensim.models import Word2Vec as GensimWord2Vec
from pyspark.sql.functions import explode, col, udf, collect_list, avg, lit
from pyspark.sql.functions import year, month, dayofmonth
from pyspark.sql.types import ArrayType, StringType, StructType, StructField, FloatType, IntegerType # Import StructType, StructField, FloatType and IntegerType

# If you need to calculate cosine similarity, you can define a function like this:
from pyspark.ml.linalg import Vectors
from pyspark.sql.functions import udf

@udf("double")
def cosine_similarity(v1, v2):
  """
  Calculates the cosine similarity between two vectors.
  Args:
      v1: First vector.
      v2: Second vector.
  Returns:
      The cosine similarity between the two vectors.
  """
  return float(v1.dot(v2) / (v1.norm(2) * v2.norm(2)))

# Define schema for business_df
business_schema = StructType([
    StructField("business_id", StringType(), True),
    StructField("name", StringType(), True),
    StructField("address", StringType(), True),
    StructField("city", StringType(), True),
    StructField("state", StringType(), True),
    StructField("postal_code", StringType(), True),
    StructField("latitude", FloatType(), True),
    StructField("longitude", FloatType(), True),
    StructField("stars", FloatType(), True),
    StructField("review_count", IntegerType(), True),
    StructField("is_open", IntegerType(), True),
    StructField("attributes", StringType(), True),  # You might need to adjust this based on your data
    StructField("categories", StringType(), True), # You might need to adjust this based on your data
    StructField("hours", StringType(), True) # You might need to adjust this based on your data
])

# Load the Yelp datasets with explicit schema or schema inference hints
file_path = "/content/drive/MyDrive/yelp_business1.csv"
business_df = spark.read.csv(file_path, header=True, schema=business_schema)
business_df.show()

from pyspark.sql import SparkSession, functions as F
user_df = spark.read.csv("/content/drive/MyDrive/ProcessedCSV/final_users.csv", header=True, inferSchema=True)
columns_to_drop = user_df.columns
user_df = user_df.filter(F.col(columns_to_drop[0]) != user_df.first()[columns_to_drop[0]])
for col in columns_to_drop[1:]:
    user_df = user_df.filter(F.col(col) != user_df.first()[col])
user_df.show()

review_df = spark.read.csv("/content/drive/MyDrive/ProcessedCSV/final_review.csv", header=True, inferSchema=True)
columns_to_drop = review_df.columns
review_df = review_df.filter(F.col(columns_to_drop[0]) != review_df.first()[columns_to_drop[0]])
for col in columns_to_drop[1:]:
    review_df = review_df.filter(F.col(col) != review_df.first()[col])
review_df.show()

tip_df = spark.read.csv("/content/drive/MyDrive/ProcessedCSV/final_tip.csv", header=True, inferSchema=True)
columns_to_drop = tip_df.columns
tip_df = tip_df.filter(F.col(columns_to_drop[0]) != tip_df.first()[columns_to_drop[0]])
for col in columns_to_drop[1:]:
    tip_df = tip_df.filter(F.col(col) != tip_df.first()[col])
tip_df.show()

checkin_df = spark.read.csv("/content/drive/MyDrive/ProcessedCSV/final_checkin.csv", header=True, inferSchema=True)
columns_to_drop = checkin_df.columns
checkin_df = checkin_df.filter(F.col(columns_to_drop[0]) != checkin_df.first()[columns_to_drop[0]])
for col in columns_to_drop[1:]:
    checkin_df = checkin_df.filter(F.col(col) != checkin_df.first()[col])
checkin_df.show()

from pyspark.sql import SparkSession, functions as F

# Overview of Business/Restaurant Categories

# Use withColumn to add a new column 'categories' by splitting the existing 'categories' column
business_categories = business_df.withColumn('categories', F.split(business_df['categories'], ', ')) \
                                 .withColumn('categories', F.explode('categories')) # Explode the array to get individual categories in separate rows

print(str('The number of unique business categories is:'), business_categories.count())  # Use count() to get the number of rows in PySpark DataFrame

# Most Common Business Categories
# Use groupBy and count to get the frequency of each category
business_categories.groupBy('categories').count().orderBy(F.desc('count')).show(30)

# Assuming your business data is in a DataFrame called 'business_df'

filtered_businesses = business_df.filter((F.col("review_count") >= 20) & (F.col("stars") >= 2))

# Verify the schema to check the new data type
filtered_businesses.printSchema()
filtered_businesses.show()

# Top States for business
top_states = filtered_businesses.groupBy('state').count().orderBy(F.desc('count')).limit(10)
top_states.show()

# Top cities for business
top_cities = filtered_businesses.groupBy('city').count().orderBy(F.desc('count')).limit(20)
top_cities.show()

from pyspark.sql.functions import col, isnan, when, count

# Check for null or NaN values in each column
null_counts = filtered_businesses.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in filtered_businesses.columns])

# Display the null counts for each column
null_counts.show()

# Filter reviews based on stars and year
filtered_reviews = review_df.filter((col("stars") >= 2) & (col("year") >= 2017))

# Display the filtered reviews
filtered_reviews.printSchema()
filtered_reviews.show()

first_5_user_ids = filtered_reviews.select("user_id").distinct().limit(5).rdd.flatMap(lambda x: x).collect()
print(first_5_user_ids)

# Check the schema of both DataFrames
filtered_reviews.printSchema()
filtered_businesses.printSchema()

# Check for distinct business IDs in both DataFrames
filtered_reviews.select("business_id").distinct().show()
filtered_businesses.select("business_id").distinct().show()

# Check for null business_id values
filtered_reviews.filter(col("business_id").isNull()).show()
filtered_businesses.filter(col("business_id").isNull()).show()

# Check row counts in both DataFrames
print("Number of rows in filtered_reviews:", filtered_reviews.count())
print("Number of rows in filtered_businesses:", filtered_businesses.count())

# Perform the join if both DataFrames contain data
if filtered_reviews.count() > 0 and filtered_businesses.count() > 0:
    joined_df = filtered_reviews.join(filtered_businesses, on="business_id", how="inner")
    joined_df.show(10)
else:
    print("One or both DataFrames are empty, cannot perform join.")

from pyspark.sql.functions import col, count, when, isnan, isnull, broadcast

# Rename 'stars' column in filtered_reviews before joining
filtered_businesses = filtered_businesses.withColumnRenamed("stars", "filtered_business_stars")
filtered_reviews = filtered_reviews.withColumnRenamed("stars", "filtered_review_stars")

# Perform an inner join based on 'business_id'
# Use broadcast for smaller DataFrame (filtered_businesses) to optimize join
joined_df1 = filtered_reviews.join(broadcast(filtered_businesses), on="business_id", how="inner")

# Check the number of rows in joined_df1
print("Number of rows in joined_df1:", joined_df1.count())

joined_df1.show()

from pyspark.sql.functions import col, count, when, isnan, isnull
from pyspark.sql.types import IntegerType, FloatType
from pyspark.ml.recommendation import ALS



# Check for missing or invalid values in 'filtered_review_stars'
print("Number of rows with missing 'filtered_review_stars':", joined_df1.filter(isnan("filtered_review_stars") | isnull("filtered_review_stars")).count())
print("Number of rows with zero 'filtered_review_stars':", joined_df1.filter(col("filtered_review_stars") == 0).count())

# If there are missing or invalid values, handle them appropriately (e.g., impute with average)

# Cast user_id and business_id to integers
# Use a temporary DataFrame to avoid the ambiguity
# Explicitly specify the source DataFrame for 'stars' using alias
final_df = joined_df1.select(
    col("user_id").cast(StringType()),
    col("business_id").cast(StringType()),
    col("filtered_review_stars").cast(FloatType())  # Choose which column to use
)

final_df.show()

final_df.printSchema()

# Create StringIndexer objects for user_id and business_id
user_indexer = StringIndexer(inputCol="user_id", outputCol="user_id_indexed")
business_indexer = StringIndexer(inputCol="business_id", outputCol="business_id_indexed")

# Fit the indexers and transform the DataFrame
indexed_df = user_indexer.fit(final_df).transform(final_df)
indexed_df = business_indexer.fit(indexed_df).transform(indexed_df)

# Create an ALS object with the correct column names
als = ALS(userCol="user_id_indexed", itemCol="business_id_indexed", ratingCol="filtered_review_stars", coldStartStrategy="drop")

# Fit ALS model using the indexed DataFrame
als_model = als.fit(indexed_df)

# Generate ALS recommendations (using indexed IDs)
user_als_recommendations = als_model.recommendForAllUsers(10)
item_als_recommendations = als_model.recommendForAllItems(10)

# Optionally, you can join the recommendations back with the original DataFrame
# to get the original user_id and business_id values.

print("ALS Recommendations for Users:")
user_als_recommendations.show()

print("ALS Recommendations for Businesses:")
item_als_recommendations.show()

# 2. Content-based filtering using business features
# Import the necessary class
from pyspark.ml.feature import StandardScaler
# Assemble business features such as review count, stars, etc.
# Use 'filtered_business_stars' instead of 'stars' as it is present in the DataFrame
business_feature_columns = ["review_count", "filtered_business_stars", "is_open", "latitude", "longitude"]
assembler = VectorAssembler(inputCols=business_feature_columns, outputCol="features")
business_features = assembler.transform(filtered_businesses)

# Scale features
scaler = StandardScaler(inputCol="features", outputCol="scaled_features")
scaler_model = scaler.fit(business_features)
business_features = scaler_model.transform(business_features)

# Hybrid score as a weighted sum of ALS and content-based predictions
from pyspark.sql.functions import udf, explode, col
from pyspark.sql.types import DoubleType, ArrayType, StructType, StructField, FloatType, IntegerType


# Modify hybrid_score function to handle Vector type
def hybrid_score_udf(als_prediction_array, content_features_vector, alpha=0.5):
    """
    Calculates the hybrid score.

    Args:
        als_prediction_array (list of structs): The ALS prediction rating,
                                             represented as a list of structs with `business_id_indexed` and `rating`.
        content_features_vector (pyspark.ml.linalg.SparseVector or pyspark.ml.linalg.DenseVector): The content-based features vector.
        alpha (float, optional): The weight given to ALS predictions. Defaults to 0.5.

    Returns:
        list of structs:  A list of structs with `business_id_indexed`, `rating` and  `hybrid_score`.
    """
    hybrid_scores = []

    for prediction in als_prediction_array:
        als_rating = prediction["rating"]
        business_id_indexed = prediction["business_id_indexed"]
        # Extract the first element (or desired feature) from the content_features_vector
        content_feature = content_features_vector[0]  # You may need to adjust this based on the desired feature
        hybrid_score_val = (alpha * als_rating) + ((1 - alpha) * content_feature)

        hybrid_scores.append({"business_id_indexed": business_id_indexed,
                              "rating": als_rating,
                              "hybrid_score": hybrid_score_val})

    return hybrid_scores



# Register the hybrid_score_udf as a UDF
hybrid_score = udf (hybrid_score_udf, ArrayType(StructType([
    StructField("business_id_indexed", IntegerType(), True),
    StructField("rating", FloatType(), True),
    StructField("hybrid_score", FloatType(), True)
])))

# Assuming 'user_als_recommendations' contains the ALS recommendations
# Rename 'recommendations' column to avoid conflict with existing column name
user_als_recommendations = user_als_recommendations.withColumnRenamed("recommendations", "als_recommendations")

# Import necessary modules
from pyspark.ml.linalg import Vectors, VectorUDT  # Add this import
from pyspark.sql.functions import struct, collect_list, first, col # Add 'struct' here

# Check if 'business_id_indexed' already exists in 'business_features'
if 'business_id_indexed' not in business_features.columns:  # Add this condition
    business_features = business_indexer.fit(business_features).transform(business_features) # add this line to index business_id and add 'business_id_indexed' column

# Assuming 'user_als_recommendations' contains the ALS recommendations
# Rename 'recommendations' column to avoid conflict with existing column name
user_als_recommendations = user_als_recommendations.withColumnRenamed("recommendations", "als_recommendations")

# Explode the 'als_recommendations' array to get individual rows for each recommendation
# This will create a new column 'business_id_indexed' that can be used for the join
exploded_recommendations = user_als_recommendations.select(
    "user_id_indexed",
    explode("als_recommendations").alias("exploded_recommendation")
).select(
    "user_id_indexed",
    "exploded_recommendation.business_id_indexed",
    "exploded_recommendation.rating"
)

# Now you can join using the 'business_id_indexed' column
hybrid_recommendations = exploded_recommendations.join(business_features, on="business_id_indexed", how="inner")

# **Change here**: Group by 'user_id_indexed' and collect ALS recommendations and scaled features into lists
hybrid_recommendations = hybrid_recommendations.groupBy("user_id_indexed").agg(
    collect_list(struct("business_id_indexed", "rating")).alias("als_recommendations"),
    first("scaled_features").alias("scaled_features") # Take the first scaled_features as it should be the same for all recommendations of a user
)

# **Change here**:  Convert the 'scaled_features' column to a plain array before calling the UDF
hybrid_recommendations = hybrid_recommendations.withColumn(
    "scaled_features_array",
    udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))("scaled_features")
)

# Calculate hybrid scores using the collected lists and the converted array
hybrid_recommendations = hybrid_recommendations.withColumn(
    "hybrid_scores",
    hybrid_score(col("als_recommendations"), col("scaled_features_array"))  # Pass the array here
)

hybrid_recommendations.show(5)

from pyspark.sql import functions as F
from pyspark.sql.window import Window

def calculate_rank_relevance(predictions, relevance_threshold=3):
    """
    Calculates rank and relevance for recommendations.

    Args:
        predictions (pyspark.sql.DataFrame): DataFrame containing recommendations.
        relevance_threshold (float): Threshold for determining relevance based on 'hybrid_scores'.

    Returns:
        pyspark.sql.DataFrame: DataFrame with added 'rank' and 'relevance' columns.
    """

    # Explode the hybrid_scores array to get individual rows for each recommendation
    predictions = predictions.select(
        "user_id_indexed",
        F.explode("hybrid_scores").alias("exploded_hybrid_score")
    ).select(
        "user_id_indexed",
        "exploded_hybrid_score.business_id_indexed",
        "exploded_hybrid_score.rating",
        "exploded_hybrid_score.hybrid_score"  # Extract the hybrid_score from the struct
    )

    # Create a window to rank recommendations for each user
    window = Window.partitionBy("user_id_indexed").orderBy(F.desc("hybrid_score")) # Use the extracted hybrid_score

    # Add rank column based on hybrid_scores
    predictions_with_rank = predictions.withColumn("rank", F.row_number().over(window))

    # Add relevance column based on hybrid_scores and threshold
    predictions_with_rank_relevance = predictions_with_rank.withColumn(
        "relevance", F.when(F.col("hybrid_score") > relevance_threshold, 1).otherwise(0) # Use the extracted hybrid_score
    )

    return predictions_with_rank_relevance

# Apply the function to your hybrid_recommendations DataFrame
ranked_recommendations = calculate_rank_relevance(hybrid_recommendations)

# Display the results
ranked_recommendations.show(5)

def recall_at_k(predictions, k):
    # Define the window specification for ranking
    window = Window.partitionBy("user_id_indexed").orderBy(F.desc("hybrid_score"))  # Use 'hybrid_score' instead of 'hybrid_scores'

    # Get top k recommendations for each user
    top_k_recs = predictions.withColumn("rank", F.row_number().over(window)).filter(F.col("rank") <= k)

    # Count relevant items in top k recommendations
    relevant_items_in_top_k = top_k_recs.filter(F.col("relevance") == 1).groupBy("user_id_indexed").agg(F.count("business_id_indexed").alias("relevant_count"))

    # Count total relevant items for each user
    total_relevant_items = predictions.filter(F.col("relevance") == 1).groupBy("user_id_indexed").agg(F.count("business_id_indexed").alias("total_relevant_count"))

    # Calculate recall@k
    recall = relevant_items_in_top_k.join(total_relevant_items, "user_id_indexed").withColumn("recall", F.col("relevant_count") / F.col("total_relevant_count"))

    # Return average recall@k across all users
    return recall.select(F.mean("recall")).collect()[0][0]

# NDCG@K metric
def ndcg_at_k(predictions, k):
    # Change here: Use 'user_id_indexed' for partitioning instead of 'user_id'
    # Change here: Use 'hybrid_score' instead of 'hybrid_scores' for orderBy
    predictions = predictions.withColumn("rank", F.row_number().over(Window.partitionBy("user_id_indexed").orderBy(F.desc("hybrid_score"))))

    # Replace 'actual' with the actual relevance column in your DataFrame
    # Here, I'm assuming your relevance column is named 'relevance'
    # If it's different, change it accordingly
    predictions = predictions.withColumn("dcg", F.when(F.col("rank") <= k, F.col("relevance") / F.log2(F.col("rank") + 1)))

    # Change here: Use 'user_id_indexed' for grouping instead of 'user_id'
    dcg = predictions.groupBy("user_id_indexed").agg(F.sum("dcg").alias("dcg"))
    # Change here: Use 'user_id_indexed' for grouping instead of 'user_id'
    idcg = predictions.groupBy("user_id_indexed").agg(F.sum(F.lit(1) / F.log2(F.col("rank") + 1)).alias("idcg"))
    # Change here: Use 'user_id_indexed' for joining instead of 'user_id'
    ndcg = dcg.join(idcg, "user_id_indexed").withColumn("ndcg", F.col("dcg") / F.col("idcg"))
    return ndcg.select(F.mean("ndcg")).collect()[0][0]

import pandas as pd
from pyspark.sql import functions as F

def avg_precision_at_k_per_user(ranked_recommendations, k=5, num_users_to_show=6):
    """
    Calculates average precision@k per user for ranked recommendations and shows results for a specified number of users.

    Args:
        ranked_recommendations: A PySpark DataFrame with columns:
            - user_id_indexed: The indexed user ID.
            - business_id_indexed: The indexed business ID.
            - relevance: Relevance of the business to the user (1 for relevant, 0 for irrelevant).
        k: The number of top recommendations to consider (default: 5).
        num_users_to_show: The number of users to show precision for (default: 6).

    Returns:
        A pandas DataFrame with columns:
            - user_id_indexed: The indexed user ID.
            - avg_precision_at_k: The average precision@k value for the user.
    """

    # Calculate precision@k for each user
    precision_per_user_df = ranked_recommendations.withColumn("rank", F.row_number().over(Window.partitionBy("user_id_indexed").orderBy(F.desc("hybrid_score")))) \
        .filter(F.col("rank") <= k) \
        .groupBy("user_id_indexed") \
        .agg(F.collect_list(F.struct("rank", "relevance")).alias("ranked_items")) \
        .withColumn("avg_precision_at_k", F.expr("""
            aggregate(
                zip_with(ranked_items, sequence(1, size(ranked_items))),
                cast(0 as double),
                (acc, x) -> acc + (if(x.ranked_items.relevance = 1, cast(sum(y.ranked_items.relevance for y in slice(ranked_items, 1, x.ranked_items.rank)) as double) / x.ranked_items.rank, 0.0))
            ) / sum(cast(x.ranked_items.relevance as int) for x in ranked_items)
        """)) \
        .select("user_id_indexed", "avg_precision_at_k")

    # Convert to pandas DataFrame and show results for specified number of users
    precision_df = precision_per_user_df.limit(num_users_to_show).toPandas()  # Limit to num_users_to_show

    return precision_df

from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, rank

# Diversity metric (measuring how diverse the recommendations are)
def diversity(predictions, k=None): # Add k as an optional parameter
    # If k is provided, take only the top k recommendations for each user
    if k:
        # Create a window specification to partition by user_id and order by some relevant column (e.g., prediction score)
        # CHANGED: Replaced 'user_id' with 'user_id_indexed' in partitionBy
        # CHANGED: Replaced 'prediction_score' with 'hybrid_score' or the correct score column name
        window_spec = Window.partitionBy("user_id_indexed").orderBy("hybrid_score")

        # Use row_number or rank to assign a rank to each recommendation within the user's partition
        # and filter out recommendations with rank > k
        predictions = predictions.withColumn("rank", row_number().over(window_spec)) \
                                 .filter("rank <= {}".format(k)) \
                                 .drop("rank")  # Remove the temporary rank column if needed
    # Use 'business_id_indexed' instead of 'business_id'
    unique_items = predictions.select("business_id_indexed").distinct().count()
    total_recommendations = predictions.count()
    return unique_items / total_recommendations

# Evaluate  Recall@K, NDCG, and Diversity
k = 5

recall = recall_at_k(ranked_recommendations, k)

ndcg = ndcg_at_k(ranked_recommendations, k)

diversity_score = diversity(ranked_recommendations,k)

# Printing evaluatiom metrices
print(f"Recall@{k}: {recall}")
print(f"NDCG@{k}: {ndcg}")
print(f"Diversity: {diversity_score}")

# Stop the Spark session
spark.stop()